{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUYM4ePR6jYuQ7HBLIhHoQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zooveevolbeido/br/blob/main/titanic_(binary_classification)_(manual_process).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "\n"
      ],
      "metadata": {
        "id": "-cEexXVtncBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_train = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
        "\n",
        "url_test = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\"\n"
      ],
      "metadata": {
        "id": "XuYDJn0Tnl4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv(url_train)\n",
        "\n",
        "df_test = pd.read_csv(url_test)\n"
      ],
      "metadata": {
        "id": "oUIEy6cSnsdC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################################################\n",
        "#sex\n",
        "\n",
        "gender_train = np.empty(len(df_train['sex']))\n",
        "\n",
        "for i in range(0, len(df_train)):\n",
        "    for j in range(0, len(np.unique(df_train['sex']))):\n",
        "        if df_train['sex'][i] == np.unique(df_train['sex'])[j]:\n",
        "            gender_train[i] = j\n",
        "\n",
        "df_train = df_train.assign(SEX=gender_train)\n",
        "#print(df_train)\n",
        "\n",
        "###################################\n",
        "\n",
        "gender_test = np.empty(len(df_test['sex']))\n",
        "\n",
        "for i in range(0, len(df_test)):\n",
        "    for j in range(0, len(np.unique(df_test['sex']))):\n",
        "        if df_test['sex'][i] == np.unique(df_test['sex'])[j]:\n",
        "            gender_test[i] = j\n",
        "\n",
        "#print(gender_test)\n",
        "df_test = df_test.assign(SEX = gender_test)\n",
        "#print(df_test)\n",
        "\n",
        "\n",
        "\n",
        "########################################################\n",
        "#class\n",
        "\n",
        "class_train = np.empty(len(df_train['class']))\n",
        "\n",
        "for i in range(0, len(df_train)):\n",
        "    for j in range(0, len(np.unique(df_train['class']))):\n",
        "        if df_train['class'][i] == np.unique(df_train['class'])[j]:\n",
        "            class_train[i] = j\n",
        "\n",
        "df_train = df_train.assign(CLASS=class_train)\n",
        "#print(df_train)\n",
        "\n",
        "###################################\n",
        "\n",
        "class_test = np.empty(len(df_test['class']))\n",
        "\n",
        "for i in range(0, len(df_test)):\n",
        "    for j in range(0, len(np.unique(df_test['class']))):\n",
        "        if df_test['class'][i] == np.unique(df_test['class'])[j]:\n",
        "            class_test[i] = j\n",
        "\n",
        "#print(gender_test)\n",
        "df_test = df_test.assign(CLASS = class_test)\n",
        "#print(df_test)\n",
        "\n",
        "\n",
        "\n",
        "##################################################################\n",
        "#deck\n",
        "\n",
        "deck_train = np.empty(len(df_train['deck']))\n",
        "\n",
        "for i in range(0, len(df_train)):\n",
        "    for j in range(0, len(np.unique(df_train['deck']))):\n",
        "        if df_train['deck'][i] == np.unique(df_train['deck'])[j]:\n",
        "            deck_train[i] = j\n",
        "\n",
        "df_train = df_train.assign(DECK=deck_train)\n",
        "#print(df_train)\n",
        "\n",
        "###################################\n",
        "\n",
        "deck_test = np.empty(len(df_test['deck']))\n",
        "\n",
        "for i in range(0, len(df_test)):\n",
        "    for j in range(0, len(np.unique(df_test['deck']))):\n",
        "        if df_test['deck'][i] == np.unique(df_test['deck'])[j]:\n",
        "            deck_test[i] = j\n",
        "\n",
        "#print(gender_test)\n",
        "df_test = df_test.assign(DECK = deck_test)\n",
        "#print(df_test)\n",
        "\n",
        "\n",
        "####################################################################\n",
        "#embark_town\n",
        "\n",
        "embark_town_train = np.empty(len(df_train['embark_town']))\n",
        "\n",
        "for i in range(0, len(df_train)):\n",
        "    for j in range(0, len(np.unique(df_train['embark_town']))):\n",
        "        if df_train['embark_town'][i] == np.unique(df_train['embark_town'])[j]:\n",
        "            embark_town_train[i] = j\n",
        "\n",
        "df_train = df_train.assign(EMBARK_TOWN = embark_town_train)\n",
        "#print(df_train)\n",
        "\n",
        "###################################\n",
        "\n",
        "embark_town_test = np.empty(len(df_test['embark_town']))\n",
        "\n",
        "for i in range(0, len(df_test)):\n",
        "    for j in range(0, len(np.unique(df_test['embark_town']))):\n",
        "        if df_test['embark_town'][i] == np.unique(df_test['embark_town'])[j]:\n",
        "            embark_town_test[i] = j\n",
        "\n",
        "#print(gender_test)\n",
        "df_test = df_test.assign(EMBARK_TOWN = embark_town_test)\n",
        "#print(df_test)\n",
        "\n",
        "\n",
        "#########################################################################\n",
        "#alone\n",
        "\n",
        "\n",
        "alone_train = np.empty(len(df_train['alone']))\n",
        "\n",
        "for i in range(0, len(df_train)):\n",
        "    for j in range(0, len(np.unique(df_train['alone']))):\n",
        "        if df_train['alone'][i] == np.unique(df_train['alone'])[j]:\n",
        "            alone_train[i] = j\n",
        "\n",
        "df_train = df_train.assign(ALONE=alone_train)\n",
        "#print(df_train)\n",
        "\n",
        "###################################\n",
        "\n",
        "alone_test = np.empty(len(df_test['alone']))\n",
        "\n",
        "for i in range(0, len(df_test)):\n",
        "    for j in range(0, len(np.unique(df_test['alone']))):\n",
        "        if df_test['alone'][i] == np.unique(df_test['alone'])[j]:\n",
        "            alone_test[i] = j\n",
        "\n",
        "#print(gender_test)\n",
        "df_test = df_test.assign(ALONE = alone_test)\n",
        "#print(df_test)\n"
      ],
      "metadata": {
        "id": "h0b4nOX1nzAv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['sex','class','deck', 'embark_town', 'alone'], axis=1)\n",
        "\n",
        "#print(df_train)\n",
        "\n",
        "df_test = df_test.drop(['sex','class','deck', 'embark_town', 'alone'], axis=1)\n"
      ],
      "metadata": {
        "id": "iqRWKECLov4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DF = pd.concat([df_train, df_test], axis=0)\n",
        "\n",
        "#print(DF)\n",
        "\n",
        "X = DF.drop(['survived'], axis=1).values\n",
        "y = DF['survived'].values\n",
        "\n",
        "print(X.shape)\n",
        "print(y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQOFt1Ugo5sv",
        "outputId": "61f4de6a-452b-4edd-8d17-0c68c27c1706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(891, 9)\n",
            "(891,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=76)"
      ],
      "metadata": {
        "id": "FmQIrp5QpA0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "'''\n",
        "nonlinear_train = (X_train)**1.5\n",
        "X_train = np.concatenate((X_train, nonlinear_train), axis=1)\n",
        "\n",
        "nonlinear_test = X_test**1.5\n",
        "X_test = np.concatenate((X_test, nonlinear_test), axis=1)\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "VlVT5vUM9ype",
        "outputId": "a8749c57-c045-4c8a-ecfe-98b7d036bba7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nnonlinear_train = (X_train)**1.5\\nX_train = np.concatenate((X_train, nonlinear_train), axis=1)\\n\\nnonlinear_test = X_test**1.5\\nX_test = np.concatenate((X_test, nonlinear_test), axis=1)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 505
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def overSampling(X_train, y_train):\n",
        "\n",
        "    n_class = len(np.unique(y_train) )\n",
        "\n",
        "    con = []\n",
        "\n",
        "    for _ in range(n_class):\n",
        "        con.append([])\n",
        "\n",
        "    for i in range(len(y_train)):\n",
        "        con[ int(y_train[i])   ].append(i)\n",
        "\n",
        "\n",
        "    freqC = []\n",
        "\n",
        "    for c in range(n_class):\n",
        "        freqC.append(len(con[c]))\n",
        "\n",
        "    #print(freqC)\n",
        "\n",
        "    scarcity = []\n",
        "\n",
        "    for c in range(n_class):\n",
        "        scarcity.append( max(freqC) - freqC[c]  )\n",
        "    print(scarcity)\n",
        "\n",
        "\n",
        "    add_list = []\n",
        "\n",
        "    for c in range(n_class):\n",
        "        if scarcity[c] > 0:\n",
        "            add_list.extend(np.random.choice(con[c], scarcity[c], replace=True))\n",
        "\n",
        "    arrsX = []\n",
        "    arrsy = []\n",
        "    for i in add_list:\n",
        "        arrsX.append(X_train[i])\n",
        "        arrsy.append(y_train[i])\n",
        "\n",
        "    X_train = np.append(X_train, arrsX, axis=0)\n",
        "    y_train = np.append(y_train, arrsy, axis=0)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    return X_train, y_train\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "overSample = overSampling(X_train, y_train)\n",
        "X_train = overSample[0]\n",
        "y_train = overSample[1]\n",
        "print(len(X_train))\n",
        "print(len(y_train))\n",
        "\n",
        "'''\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "esA89IsWynqx",
        "outputId": "e913542c-5f97-43b0-a08f-8f63d44a8081"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\noverSample = overSampling(X_train, y_train)   \\nX_train = overSample[0]\\ny_train = overSample[1]\\nprint(len(X_train))\\nprint(len(y_train))\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 506
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "Lzf-e1Psq2rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=( len(X_train[0]), ) )    )\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu' ))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "'''\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "'''\n",
        "\n",
        "#model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "#model.add(tf.keras.layers.Dropout(0.2))\n",
        "\n",
        "#model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "#model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "cbxpsYpPpHTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "htr = model.fit(X_train, y_train, epochs=40, batch_size=32 ,validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QcU-y1l9pixk",
        "outputId": "ab07dc35-2fca-4486-9b2c-6f93e242b043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "23/23 [==============================] - 3s 14ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 2/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 3/40\n",
            "23/23 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 4/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 5/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 6/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 7/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 8/40\n",
            "23/23 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 9/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 10/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 11/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 12/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 13/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 14/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 15/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 16/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 17/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 18/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 19/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 20/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 21/40\n",
            "23/23 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 22/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 23/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 24/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 25/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 26/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 27/40\n",
            "23/23 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 28/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 29/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 30/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 31/40\n",
            "23/23 [==============================] - 0s 6ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 32/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 33/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 34/40\n",
            "23/23 [==============================] - 0s 5ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 35/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 36/40\n",
            "23/23 [==============================] - 0s 7ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 37/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 38/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 39/40\n",
            "23/23 [==============================] - 0s 8ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n",
            "Epoch 40/40\n",
            "23/23 [==============================] - 0s 9ms/step - loss: nan - accuracy: 0.6264 - val_loss: nan - val_accuracy: 0.5754\n"
          ]
        }
      ]
    }
  ]
}